# LiteLLM Configuration

# Model list - defines available models and their configurations
model_list:
  - model_name: flash-2.0
    litellm_params:
      model: gemini/gemini-2.0-flash
      api_key: ${GEMINI_API_KEY}  # LiteLLM will read this from environment variable

  - model_name: claude-3-haiku
    litellm_params:
      model: anthropic/claude-3-haiku-20240307
      api_key: ${ANTHROPIC_API_KEY}

# General settings
general_settings:
  telemetry: false  # Disable LiteLLM telemetry
  # Set any global configurations for LiteLLM here

# Fallback settings
# fallbacks: 
#   - model: gpt-3.5-turbo
#     fallback_models: ["claude-3-haiku"]

# You can add more configuration options as your application grows
# See https://docs.litellm.ai/docs/proxy/configs for all available options 